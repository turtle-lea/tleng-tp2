En esta sección explicaremos cada parte implementada para realizar los procedimientos requeridos. Contaremos dudas, errores que fueron surgiendo y explicaremos las decisiones tomadas. Para eso vamos a dividirlo en cuatro secciones que detallamos a continuación ordenadas de las formas en las que lo fuimos realizando.

\subsection{Gramática}
No contamos con demasiadas dificultades, miramos cada paso de la descripción de la partitura y fuimos creando las producciones necesarias. Se explicita en la sección Gramática.

\subsection{PLY}
En nuestra implementación utilizamos las herramientas brindadas por PLY como explicamos en la introducción, estas son:
\subsubsection{Lexer}
Vamos a utilizar un lexer para poder tener definidas nuestras expresiones regulares y poder decidir que cadenas son o no válidas según nuestra gramática.
Para generar un lexer vamos a realizar lo descripto en el siguiente link \url{www.dc.uba.ar/materias/tl/2015/c1/files/tp2-clase-intro-a-ply/at_download/file}.
Crearemos un archivo lexer\_rules.py, definiendo los tokens y las reglas.
La lista de tokens será explicitada en la sección de la gramática.\newline
Luego pasamos a definir las reglas para cada expresión regular que deseamos caputar. En esta parte tuvimos bastantes problemas, principalmente porque optamos por usar reglas 'simples' en todas las reglas.\newline
El primero de los problemas fue al hacer la siguiente expresión regular "$(do|re|mi|fa|sol|la|si)(+|-)?$". Al tenerla toda junta, la regla matcheaba con todas las notas hasta el final del primer paréntesis sin mirar lo que continuaba y, en caso de tener en el archivo de entrada "sol+" en una nota, buscara una regla que inicialice con un + o - y no la encontrará lógicamente. La solución fue separarlas, pudiendo solucionar el problema mencionado. \newline
El segundo problema que surgió fue el tema del orden, al tener reglas como const que generan todo el alfabeto, nos matcheaba voz, compas, entre otras, cuando nosotros en realidad queriamos que esas palabras matcheen en otra regla definida. Como nosotros teníamos definido reglas 'simples', al tratar de ordenar las reglas notamos que la lógica de la clase re (regular expression) tomaba como primera a la que definía en su regla el string más largo, esto nos hizo reever la forma de definir las reglas. Mirando y testeando la clase re, pudimos corroborar que, definiendo las reglas como funciones, se respetaba el orden en las que eran definidas en el script. De esta manera pasamos todas las reglas a funciones como se explica en el pdf del link para reglas 'complejas', logrando salvar dicho problema.
\subsubsection{Parser}
Vamos a utilizar un parser para poder darle una estructura y una semántica a nuestra grámatica.
Para generar un parser vamos a realizar lo descripto en el link mencionado en el lexer desde la página 15 en adelante. Creamos un archivo parser\_rules.py, en el cual vamos a definir las producciones de la gramática y cómo generar el arbol AST (abstract Syntax Tree). Este archivo usará los tokens de lexer\_rules para construir las producciones, diferenciarlas una de las otras y filtrar aquellas que no sean válidas.\newline
Cada producción llamará a su función interna, que estará definida en parserobject.py, formando el árbol AST desde las hojas hasta su raiz. Cada una de ellas creará un objeto nodo y usará atributos sintetizados para poder intercambiar valores de una rama a la otra y poder validar las condiciones especificadas en nuestro lenguaje, en otras palabras le estaremos dando una semántica a nuestras producciones.