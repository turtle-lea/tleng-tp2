En esta sección explicaremos cada parte implementada para realizar los procedimientos requeridos. Contaremos dudas, errores que fueron surgiendo y explicaremos las decisiones tomadas. Para eso vamos a dividirlo en cuatro secciones que detallamos a continuación ordenadas de las formas en las que lo fuimos realizando.

\subsection{Gramática}
No contamos con demasiadas dificultades, miramos cada paso de la descripción de la partitura y fuimos creando las producciones necesarias. Se explicita en la sección Gramática.

\subsection{PLY}
En nuestra implementación utilizamos las herramientas brindadas por PLY como explicamos en la introducción, estas son:
\subsubsection{Lexer}
Vamos a utilizar un lexer para poder tener definidas nuestras expresiones regulares y poder decidir que cadenas son o no válidas según nuestra gramática.
Para generar un lexer vamos a realizar lo descripto en el siguiente link \url{www.dc.uba.ar/materias/tl/2015/c1/files/tp2-clase-intro-a-ply/at_download/file}.
Crearemos un archivo lexer\_rules.py, definiendo los tokens y las reglas.
La lista de tokens será explicitada en la sección de la gramática.\newline
Luego pasamos a definir las reglas para cada expresión regular que deseamos tener. En esta parte tuvimos bastantes problemas, principalmente porque optamos por usar reglas 'simples' en todas las reglas.\newline

El primero de los problemas fue al hacer la siguiente expresión regular "$(do|re|mi|fa|sol|la|si)(+|-)?$". Al tenerla toda junta, la regla matcheaba con todas las notas hasta el final del primer paréntesis sin mirar lo que continuaba y, en caso de tener en el archivo de entrada una nota con "$sol+$", buscaba una regla que inicialice con un $+$ o $-$, la cual no existe, y por lo tanto producía un error. La solución fue separarlas, pudiendo solucionar el problema mencionado. \newline

El segundo problema que surgió fue el tema del orden, al tener reglas como const que generan todo el alfabeto, nos matcheaba voz, compas, entre otras, cuando nosotros en realidad queriamos que esas palabras matcheen en otra regla definida. Como nosotros teníamos definido reglas 'simples', al tratar de ordenar las reglas notamos que la lógica de la clase re (regular expression) tomaba como primera a la que definía en su regla el string más largo, esto nos hizo rever la forma de definir las reglas. Mirando y testeando la clase re, pudimos corroborar que, definiendo las reglas como funciones, se respetaba el orden en las que eran definidas en el script. De esta manera pasamos todas las reglas a funciones como se explica en el pdf del link para reglas 'complejas', logrando salvar dicho problema.\newline

Aún habiendo solucionado los problemas anteriores, surgió un tercer problema. En este caso, se desató un problema con las palabras reservadas. Dado que establecimos un orden previamente, habíamos optado por dejar a la regla $cname$ (que es nuestro constructor de constantes) en el último lugar, y que reglas que usaban palabras como $re$, $silencio$, entre otras, se lograra matchear más arriba. Esto produjo que, al definir constantes con el nombre $fantastico$, y al tener reservada la palabra $fa$, nos imposibilitara usarla produciendo un error. Investigando un poco sobre PLY, logramos encontrar el siguiente enlace \url{http://www.dabeaz.com/ply/ply.html}, el cual habla del problema mencionado y describe como corregirlo. La solución consiste en no definir las palabras reservadas como reglas separadas, sino todo junto dentro de, en nuestro caso, $cname$. De esta manera, toda palabra que inicialice con un caracter del alfabeto va entrar por la regla $cname$, detectará la palabra que tenga que detectar y derivará el token correspondiente segun nuestra lista de $reserved$. Se puede apreciar fácilmente que en nuestro ejemplo mencionado, $fantastico$ sólo va a ser derivado a un token $CONST$ mientras que $fa$ será derivado a un token $NOTENAME$, solucionando el problema descripto.

\subsubsection{Parser}
Vamos a utilizar un parser para poder darle una estructura y una semántica a nuestra grámatica.
Para generar un parser vamos a realizar lo descripto en el link mencionado en el lexer desde la página 15 en adelante. Creamos un archivo parser\_rules.py, en el cual vamos a definir las producciones de la gramática y cómo generar el arbol AST (abstract Syntax Tree). Este archivo usará los tokens de lexer\_rules para construir las producciones, diferenciarlas una de las otras y filtrar aquellas que no sean válidas.\newline
Cada producción llamará a su función interna, que estará definida en parserobject.py, formando el árbol AST desde las hojas hasta su raiz. Cada una de ellas creará un objeto nodo y usará atributos sintetizados para poder intercambiar valores de una rama a la otra y poder validar las condiciones especificadas en nuestro lenguaje, en otras palabras le estaremos dando una semántica a nuestras producciones.